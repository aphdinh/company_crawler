{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, HttpUrl, Field\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def get_page_content(url, timeout=10):\n",
    "    \"\"\"\n",
    "    Fetch webpage content with robust error handling and retry mechanism.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to fetch\n",
    "        timeout (int): Request timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object or None\n",
    "    \"\"\"\n",
    "    # Configure retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=3,  # Total number of retries\n",
    "        backoff_factor=0.1,  # Exponential backoff\n",
    "        status_forcelist=[500, 502, 503, 504, 403, 429],  # Retry on these status codes\n",
    "        allowed_methods=[\"GET\"]  # Replace method_whitelist with allowed_methods\n",
    "    )\n",
    "    \n",
    "    # Create a session with retry and timeout\n",
    "    session = requests.Session()\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    # Custom headers to mimic a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'DNT': '1',  # Do Not Track Request Header\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send GET request with custom headers and timeout\n",
    "        response = session.get(\n",
    "            url, \n",
    "            headers=headers, \n",
    "            timeout=timeout,\n",
    "            verify=True  # SSL certificate verification\n",
    "        )\n",
    "        \n",
    "        # Raise an exception for bad status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse and return BeautifulSoup object\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logging.error(f\"HTTP error occurred: {http_err}\")\n",
    "        logging.error(f\"Status code: {response.status_code}\")\n",
    "        logging.error(f\"Response content: {response.text}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        logging.error(f\"Error connecting: {conn_err}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        logging.error(f\"Timeout error: {timeout_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        logging.error(f\"Unexpected error: {req_err}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error fetching {url}: {str(e)}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = str(get_page_content(\"https://tinyseed.com/portfolio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_scraping_method(html_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the HTML content and suggest the most appropriate scraping method.\n",
    "    \n",
    "    Args:\n",
    "        html_content (BeautifulSoup or str): HTML content of the portfolio page\n",
    "    \n",
    "    Returns:\n",
    "        dict: Suggestions for scraping method and confidence\n",
    "    \"\"\"\n",
    "    # Convert to BeautifulSoup if it's a string\n",
    "    if isinstance(html_content, str):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    else:\n",
    "        soup = html_content\n",
    "    \n",
    "    # Analysis criteria\n",
    "    analysis = {\n",
    "        'individual_urls': {\n",
    "            'score': 0,\n",
    "            'indicators': [\n",
    "                soup.find_all('a', href=re.compile(r'/company/|/portfolio/|/startup/')),\n",
    "                soup.select('a[href*=\"company\"]'),\n",
    "                soup.find_all(class_=re.compile(r'company-link|portfolio-item'))\n",
    "            ]\n",
    "        },\n",
    "        'inline_list': {\n",
    "            'score': 0,\n",
    "            'indicators': [\n",
    "                soup.find_all(class_=re.compile(r'company-list|portfolio-companies|company-grid')),\n",
    "                soup.find_all('table', class_=re.compile(r'companies')),\n",
    "                soup.find_all('div', class_=re.compile(r'company-info|portfolio-item'))\n",
    "            ]\n",
    "        },\n",
    "        'text_extraction': {\n",
    "            'score': 0,\n",
    "            'indicators': [\n",
    "                soup.find_all(['p', 'div'], text=re.compile(r'\\b[A-Z][a-z]+ (Inc\\.|LLC|Technologies|Company)\\b')),\n",
    "                soup.find_all(class_=re.compile(r'description|company-text'))\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate scores\n",
    "    for method, data in analysis.items():\n",
    "        # Sum the lengths of found elements for each indicator\n",
    "        data['score'] = sum(len(indicator) for indicator in data['indicators'] if isinstance(indicator, list))\n",
    "    \n",
    "    # Determine best method\n",
    "    best_method = max(analysis, key=lambda x: analysis[x]['score'])\n",
    "    \n",
    "    # Prepare detailed suggestion\n",
    "    suggestion = {\n",
    "        'method': best_method,\n",
    "        'confidence': min(analysis[best_method]['score'] / 10, 1.0),\n",
    "        'details': {\n",
    "            'individual_urls_count': len([url for group in analysis['individual_urls']['indicators'] for url in group]),\n",
    "            'inline_list_count': len([item for group in analysis['inline_list']['indicators'] for item in group]),\n",
    "            'text_extraction_count': len([text for group in analysis['text_extraction']['indicators'] for text in group])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return suggestion\n",
    "\n",
    "# Example usage\n",
    "def analyze_portfolio_page(html_content):\n",
    "    suggestion = suggest_scraping_method(html_content)\n",
    "    \n",
    "    print(\"Suggested Scraping Method:\")\n",
    "    print(f\"Method: {suggestion['method']}\")\n",
    "    print(f\"Confidence: {suggestion['confidence']:.2%}\")\n",
    "    print(\"Detailed Indicators:\")\n",
    "    for key, value in suggestion['details'].items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    return suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60603/3245973795.py:38: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  soup.find_all(['p', 'div'], text=re.compile(r'\\b[A-Z][a-z]+ (Inc\\.|LLC|Technologies|Company)\\b')),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'method': 'individual_urls',\n",
       " 'confidence': 0.2,\n",
       " 'details': {'individual_urls_count': 2,\n",
       "  'inline_list_count': 0,\n",
       "  'text_extraction_count': 1}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_scraping_method(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_scraping_method(html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
